{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from src.datasets import ContentWise, RL4RS, OpenCDP\n",
    "from src.utils import get_train_val_test_tmatrix_tnumitems\n",
    "\n",
    "pkl_path = '../pkl/'\n",
    "data_path = '../data/recsys/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ContentWise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading files. . . \n",
      "preprocessing. . . \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 329870/329870 [00:11<00:00, 29745.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biulding affinity matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "272907it [01:49, 2490.35it/s]\n"
     ]
    }
   ],
   "source": [
    "c_path = os.path.join( data_path, 'ContentWiseImpressions/data/ContentWiseImpressions/CW10M-CSV/')\n",
    "\n",
    "c = ContentWise(c_path, min_session_len=3, max_session_len=100)\n",
    "c.dump(os.path.join(pkl_path, 'cw.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_NO = 0\n",
    "\n",
    "# loading pickled dataset, gettin data loaders\n",
    "c = ContentWise.load(os.path.join(pkl_path, 'cw.pkl'))\n",
    "( \n",
    "    c_train_loader, \n",
    "    c_val_loader, \n",
    "    c_test_loader, \n",
    "    c_train_user_item_matrix, \n",
    "    train_num_items,\n",
    ") = get_train_val_test_tmatrix_tnumitems(c, batch_size=2, seed=123)\n",
    "\n",
    "# get batch from test loader\n",
    "i = 0\n",
    "for batch in c_test_loader:\n",
    "    if i < BATCH_NO:\n",
    "        i += 1\n",
    "        continue\n",
    "    break\n",
    "\n",
    "for key, value in batch.items():\n",
    "    print(f'\\n == {key} ==')\n",
    "    print(value)\n",
    "\n",
    "# get users\n",
    "user_id = batch['user_ids'][0][0].item(), batch['user_ids'][1][0].item()\n",
    "user_idx = batch['user_indexes'][0][0].item(), batch['user_indexes'][1][0].item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# response counts user 1: carefully check manually\n",
    "for i, user in enumerate(user_idx):\n",
    "    print(f'\\nuser {user}\\n')\n",
    "    print(c_train_user_item_matrix[user, :])\n",
    "    items = batch['slates_item_indexes'][i][batch['responses'][i] > 0]\n",
    "    counts = batch['responses'][i][batch['responses'][i] > 0]\n",
    "    print(torch.stack([items, counts]).T)\n",
    "\n",
    "# batches are collated OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking dataset: search random user\n",
    "USER_ID = 1223\n",
    "DATA_POINT = -1\n",
    "for i, user in enumerate(c):\n",
    "    if user['user_ids'][0]==USER_ID:\n",
    "        DATA_POINT = i\n",
    "c[DATA_POINT]['slates_item_ids'], c[DATA_POINT]['responses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually compare with tables\n",
    "ratings = pd.read_csv(\n",
    "    os.path.join(c_path, 'interactions.csv.gz'), \n",
    "    compression='gzip', \n",
    "    delimiter=','\n",
    ")\n",
    "ratings[(ratings.user_id == USER_ID) & (ratings.recommendation_id != -1)].sort_values('utc_ts_milliseconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.metadata[c.metadata['user_id'] == USER_ID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.responses[c.metadata[c.metadata['user_id'] == USER_ID].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slates_log = pd.read_csv(\n",
    "    os.path.join(c_path, 'impressions-direct-link.csv.gz'),\n",
    "    compression='gzip', \n",
    "    delimiter=','\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = ratings[(ratings.user_id == USER_ID) & (ratings.recommendation_id != -1)].recommendation_id.unique()\n",
    "slates_log[slates_log.recommendation_id.isin(recommendations)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL4RS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_path = os.path.join(data_path, 'rl4rs-dataset/')\n",
    "r = RL4RS(\n",
    "    r_path,\n",
    "    which='rl4rs_dataset_b_sl.csv', \n",
    "    min_session_len=3\n",
    ")\n",
    "r.dump(os.path.join(pkl_path, 'rl4rs.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = RL4RS.load(os.path.join(pkl_path, 'rl4rs.pkl'))\n",
    "DATA_POINT = 1234\n",
    "session = r[DATA_POINT]['session']\n",
    "for key, value in r[DATA_POINT].items():    \n",
    "    print(f'\\n == {key} ==')\n",
    "    print(value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    os.path.join(r_path,'rl4rs_dataset_b_sl.csv'),\n",
    "    delimiter='@'\n",
    ")\n",
    "df[df['session_id'] == session ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # OpenCDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosmetics_path = os.path.join(data_path, 'OpenCDP/cosmetics')\n",
    "for w_size in [5, 10, 20]:\n",
    "    for a_hrs in [1, 2, 4, 8, 24]:\n",
    "        o = OpenCDP(\n",
    "            cosmetics_path, \n",
    "            min_session_len=3 * w_size, # early crop sessions with < 3 user-recsys iteractions\n",
    "            window_size=w_size, \n",
    "            max_active_hours=a_hrs,\n",
    "        )\n",
    "        o.dump(os.path.join(pkl_path, f'cosmetics_{w_size}_{a_hrs}.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It is too large to handle it simultaneously -- using only first month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_path = os.path.join(data_path, 'OpenCDP/multi') \n",
    "for w_size in [5, 10, 20]:\n",
    "    for a_hrs in [1, 2, 4, 8, 24]:\n",
    "        o = OpenCDP(\n",
    "            multi_path,\n",
    "            files = ['2019-Dec.csv'],\n",
    "            min_session_len=3 * w_size, # early crop sessions with < 3 user-recsys iteractions\n",
    "            window_size=w_size, \n",
    "            max_active_hours=a_hrs,\n",
    "        )\n",
    "        o.dump(os.path.join(pkl_path, f'multi_{w_size}_{a_hrs}.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = multi_path\n",
    "files = None \n",
    "\n",
    "files = os.listdir(data_dir) if files is None else files\n",
    "events = [pd.read_csv(os.path.join(data_dir, file)) for file in files]\n",
    "raw_events = pd.concat(events)\n",
    "\n",
    "# there are separate sessions for single user,\n",
    "raw_events.user_id.count(), raw_events.user_session.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_events.event_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# чтобы собрать хоть какие-то выдачи, нам надо хотя-бы несколько десятков событий для каждого юзера\n",
    "c = raw_events[[\"user_session\",\"event_time\"]].groupby('user_session').count()\n",
    "c[(c.event_time < 300) & (c.event_time > 30)].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# практически все сессии из одного взаимодействия, придется их дропнуть. \n",
    "MIN_SESSION_LEN = 30\n",
    "good_sessions = c[c.event_time > 30].index\n",
    "print(len(good_sessions))\n",
    "raw_events = raw_events[raw_events.user_session.isin(good_sessions)]\n",
    "raw_events.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cart > view wtf?\n",
    "raw_events.event_type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Из чата:**\n",
    "1. Берем в качестве \"рекомендаций\" просмотры, а в качестве \"отклика\"— добавления в корзину. Конверсия из корзины в покупки — тоже можно, но звучит гораздо менее естественно, основной вариант первый.\n",
    "\n",
    "2. Объединяем наши \"рекомендации\" в выдачи двумя вариантами:\n",
    "* либо по дням, типа **\"выдача\" —  это все просмотренные за день товары**\n",
    "* либо окном фиксированно длины. **\"выдача\" — это последние M просмтренных айтемов**\n",
    "* либо (подумал уже после обсуждения) объединяя оба варианта, то есть \"выдачей\" считаются те айтемы, которые просмотрены не слишком много времени назад, и при этом не слишком много айтемов назад\n",
    "\n",
    "3. Пробуем все варианты, играем размером окна/периодом/etc, смотрим при каком получаются самые адекватные размеры \"выдач\" и \"сессий\"\n",
    "\n",
    "Все это, конечно, в каком-то смысле манипуляция данными, но на безрыбье (при отсутсвии других датасетов с явно залогированными выдачами), выбирать не приходится. Заняться этим планируется уже после формального дедлайна, так как у нас есть два датасета с явными выдачами, и мы считаем метрики на них. Потом просто добавится чисел на новых данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "separator = 'fixed' # period\n",
    "\n",
    "# фиксированное окно из k предметов\n",
    "WINDOW_SIZE = 8\n",
    "# число часов, в течение которых клики понимаются как отдельная \"выдача\"\n",
    "ACTIVE_HOURS = 24\n",
    "\n",
    "recommendations = []\n",
    "responses = []\n",
    "metadata = []\n",
    "idx = 0   \n",
    "\n",
    "for session, group in tqdm(raw_events.groupby('user_session')):\n",
    "    rec = []\n",
    "    clicks = []\n",
    "    for timestamp, row in group.sort_values(by='event_time').iterrows():\n",
    "        if row.event_type == 'remove_from_cart' or row.event_type == 'purchase':\n",
    "            continue\n",
    "        \n",
    "        # сохраняем время первого взаимодействия в \"выдаче\"\n",
    "        if len(rec) == 0:\n",
    "            first_action = datetime.strptime(row.event_time, \"%Y-%m-%d %H:%M:%S UTC\")\n",
    "        # добавляем событие просмотра /  корзины\n",
    "        if row.event_type == 'cart':\n",
    "            if row.product_id not in rec:\n",
    "                rec.append(row.product_id)\n",
    "                clicks.append(1)\n",
    "            else:\n",
    "                clicks[rec.index(row.product_id)] += 1\n",
    "        elif row.event_type == 'view':\n",
    "            rec.append(row.product_id)\n",
    "            clicks.append(0)\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "        # ecли прошло слишком много времени внутри выдачи, закончим ее\n",
    "        if (datetime.strptime(row.event_time, \"%Y-%m-%d %H:%M:%S UTC\") -\n",
    "                first_action >\n",
    "                timedelta(hours=ACTIVE_HOURS)):\n",
    "            rec.extend([-1] * (WINDOW_SIZE - len(rec)))\n",
    "            clicks.extend([0] * (WINDOW_SIZE - len(clicks)))\n",
    "            \n",
    "        if len(rec) == WINDOW_SIZE:\n",
    "            metadata.append({\n",
    "                'recommendation_idx': len(recommendations),\n",
    "                'session_id': row.user_session,\n",
    "                'user_id': row.user_id,\n",
    "                'timestamp': row.event_time,    \n",
    "            })\n",
    "            recommendations.append(rec)\n",
    "            responses.append(clicks)\n",
    "            rec, clicks = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr = np.array(responses)\n",
    "rr.sum(-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets import RecommendationData\n",
    "r = RecommendationData(\n",
    "            recommendations = np.array(recommendations).astype(int),\n",
    "            responses = np.array(responses).astype(int),\n",
    "            metadata = pd.DataFrame(metadata)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(metadata)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(r)):\n",
    "    if -1 in r[i]['slates_item_ids']:\n",
    "        break\n",
    "r[i]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_events[(raw_events.user_session == '0002dda7-653c-44b3-be14-ff8857e90f1f')].sort_values('event_time').head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generated Dataset statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "\n",
    "for filename in os.listdir('../pkl'):\n",
    "    if not (\n",
    "        # filename.startswith('multi') or\\\n",
    "        filename.startswith('cosmetics')\n",
    "    ):\n",
    "        continue\n",
    "    o = OpenCDP.load(\n",
    "        os.path.join('../pkl/', filename),\n",
    "    )\n",
    "    o.metadata.set_index(['session_id', 'timestamp'], drop=False, inplace=True)\n",
    "    o.metadata.sort_index(inplace=True)\n",
    "    o.dump(os.path.join('../pkl/', filename))\n",
    "    N = len(o)\n",
    "    stats = {\n",
    "        'length': 0,\n",
    "        'real_slate_size': 0        \n",
    "    }\n",
    "    for datapoint in tqdm(o, desc=filename):\n",
    "        stats['length'] += datapoint['length']\n",
    "        stats['real_slate_size'] += datapoint['slates_mask'].sum()\n",
    "    stats['real_slate_size'] /= stats['length']\n",
    "    stats['length'] /= N\n",
    "    res[filename] = stats\n",
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
